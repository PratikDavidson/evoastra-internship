{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f6eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0b3f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-11 13:14:39.439943: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754898279.458405   12785 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754898279.463734   12785 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754898279.480297   12785 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754898279.480337   12785 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754898279.480340   12785 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754898279.480342   12785 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-11 13:14:39.485786: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_hub\n",
    "from keras._tf_keras.keras import layers\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce32e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a8073bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "IMG_DIR= os.getcwd() + \"/data/images\"\n",
    "CAPTION_FILE = os.getcwd() + \"/data/captions.json\"\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "MAX_TOKENS = 5000\n",
    "SEQ_LEN = 32\n",
    "VOCAB_SIZE = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0874e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open(CAPTION_FILE, 'r') as f:\n",
    "        captions_data = json.load(f)\n",
    "\n",
    "    image_id_to_caption = {}\n",
    "    for data in captions_data:\n",
    "        img_id = data[\"image_id\"]\n",
    "        caption = data[\"caption\"]\n",
    "        if img_id not in image_id_to_caption:\n",
    "            image_id_to_caption[img_id] = []\n",
    "        image_id_to_caption[img_id].append(caption)\n",
    "\n",
    "    image_caption_pairs = []\n",
    "    for img_id, captions in image_id_to_caption.items():\n",
    "        filename = f\"COCO_train2014_{img_id:012d}.jpg\"\n",
    "        img_path = os.path.join(IMG_DIR, filename)\n",
    "        for caption in captions:\n",
    "            image_caption_pairs.append((img_path, f\"<START> {caption} <END>\"))\n",
    "    \n",
    "    return image_caption_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4adb8511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs: 93950\n"
     ]
    }
   ],
   "source": [
    "image_caption_pairs = load_data()\n",
    "print(f\"Total pairs: {len(image_caption_pairs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9673ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754746749.759671    4727 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3584 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "image_paths, captions = zip(*load_data())\n",
    "dataset = tf.data.Dataset.from_tensor_slices((list(image_paths), list(captions)))\n",
    "dataset = dataset.shuffle(len(image_caption_pairs), reshuffle_each_iteration=False)\n",
    "dataset_size = tf.data.experimental.cardinality(dataset).numpy()\n",
    "train_size = int(0.8 * dataset_size)\n",
    "train_ds = dataset.take(train_size)\n",
    "val_ds = dataset.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58bc7cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 19:27:54.695748: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "tokenizer = layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_sequence_length=SEQ_LEN,\n",
    "    standardize=\"lower_and_strip_punctuation\",\n",
    "    output_mode=\"int\"\n",
    ")\n",
    "text_data = train_ds.map(lambda img, cap: cap, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "tokenizer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ae04ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.get_vocabulary()\n",
    "with open(\"vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for token in vocab:\n",
    "        f.write(token + \"\\n\")\n",
    "\n",
    "with open(\"tokenizer_meta.json\", \"w\") as f:\n",
    "    json.dump({\"max_len\": 30, \"max_tokens\": len(vocab)}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c69e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_caption(image_path, caption):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (224, 224))\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    cap_tokens = tokenizer(caption)\n",
    "    return img, cap_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa0d04a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.map(preprocess_image_caption, num_parallel_calls=tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ds   = val_ds.map(preprocess_image_caption, num_parallel_calls=tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1c4d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder (ViT)\n",
    "vit_backbone = keras_hub.models.Backbone.from_preset(\"vit_base_patch16_224_imagenet\")\n",
    "vit_backbone.trainable = False\n",
    "\n",
    "image_input = keras.Input(shape=(224, 224, 3), name=\"image\")\n",
    "encoder_outputs = vit_backbone(image_input)\n",
    "#encoder_features = layers.GlobalAveragePooling1D()(encoder_outputs)\n",
    "#encoder_features = layers.Dense(512, activation=\"relu\")(encoder_features)\n",
    "\n",
    "# Decoder (GRU)\n",
    "caption_input = keras.Input(shape=(None,), dtype=tf.int32, name=\"caption\")\n",
    "x = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=512, mask_zero=True)(caption_input)\n",
    "attn_out = layers.Attention()([x, encoder_outputs])\n",
    "x = layers.Concatenate()([x, attn_out])\n",
    "x = layers.GRU(512, return_sequences=True)(x)\n",
    "output = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=[image_input, caption_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc89fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {i: w for i, w in enumerate(tokenizer.get_vocabulary())}\n",
    "start_token_id = tokenizer(\"<START>\").numpy()[0]\n",
    "end_token_id = tokenizer(\"<END>\").numpy()[0]\n",
    "\n",
    "class BLEUCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, val_dataset, max_len=30):\n",
    "        super().__init__()\n",
    "        self.val_dataset = val_dataset\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def decode_image(self, img_tensor):\n",
    "        \"\"\"Greedy decoding for image caption.\"\"\"\n",
    "        dec_input = tf.expand_dims([start_token_id], 0)\n",
    "        result = []\n",
    "\n",
    "        for _ in range(self.max_len):\n",
    "            preds = self.model([img_tensor, dec_input], training=False)\n",
    "            pred_id = tf.argmax(preds[:, -1, :], axis=-1).numpy()[0]\n",
    "            if pred_id == end_token_id:\n",
    "                break\n",
    "            result.append(index_to_word.get(pred_id, \"\"))\n",
    "            dec_input = tf.concat([dec_input, tf.expand_dims([pred_id], 0)], axis=1)\n",
    "\n",
    "        return \" \".join(result)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        smoothie = SmoothingFunction().method4\n",
    "        bleu_scores = []\n",
    "\n",
    "        for img_batch, cap_batch in self.val_dataset.take(10):  # limit for speed\n",
    "            for i in range(len(img_batch)):\n",
    "                img_tensor = tf.expand_dims(img_batch[i], 0)\n",
    "                pred_caption = self.decode_image(img_tensor)\n",
    "\n",
    "                gt_tokens = cap_batch[i].numpy()\n",
    "                gt_words = [index_to_word.get(idx, \"\") for idx in gt_tokens\n",
    "                            if idx not in [0, start_token_id, end_token_id]]\n",
    "\n",
    "                reference = [nltk.word_tokenize(\" \".join(gt_words))]\n",
    "                candidate = nltk.word_tokenize(pred_caption)\n",
    "\n",
    "                bleu = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
    "                bleu_scores.append(bleu)\n",
    "\n",
    "        avg_bleu = np.mean(bleu_scores)\n",
    "        logs[\"val_bleu\"] = avg_bleu\n",
    "        print(f\"\\nEpoch {epoch+1} — Val BLEU: {avg_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7dd2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6f7c7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vi_t_backbone       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">197</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">85,798,656</span> │ image[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ViTBackbone</span>)       │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ caption             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vi_t_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │ caption[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">393,728</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,575,936</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130,000</span> │ gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ image (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│                     │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vi_t_backbone       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m197\u001b[0m, \u001b[38;5;34m768\u001b[0m)  │ \u001b[38;5;34m85,798,656\u001b[0m │ image[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mViTBackbone\u001b[0m)       │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ caption             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ vi_t_backbone[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m5,120,000\u001b[0m │ caption[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │    \u001b[38;5;34m393,728\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m) │  \u001b[38;5;34m1,575,936\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │  \u001b[38;5;34m5,130,000\u001b[0m │ gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "│                     │ \u001b[38;5;34m10000\u001b[0m)            │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">98,018,320</span> (373.91 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m98,018,320\u001b[0m (373.91 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,219,664</span> (46.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,219,664\u001b[0m (46.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">85,798,656</span> (327.30 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m85,798,656\u001b[0m (327.30 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "202eca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    \"checkpoints/best_model.keras\",\n",
    "    save_best_only=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7a17c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754747885.835610    5470 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1186s\u001b[0m 501ms/step - accuracy: 0.1649 - loss: 4.7727 - val_accuracy: 0.1742 - val_loss: 3.0523\n",
      "Epoch 2/3\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1291s\u001b[0m 550ms/step - accuracy: 0.1790 - loss: 2.9458 - val_accuracy: 0.1877 - val_loss: 2.7479\n",
      "Epoch 3/3\n",
      "\u001b[1m2349/2349\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1201s\u001b[0m 511ms/step - accuracy: 0.1910 - loss: 2.6448 - val_accuracy: 0.1941 - val_loss: 2.6092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7dff7a4ad8d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds.map(lambda img, cap: ((img, cap[:, :-1]), cap[:, 1:])),\n",
    "    validation_data=val_ds.map(lambda img, cap: ((img, cap[:, :-1]), cap[:, 1:])),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb2febc",
   "metadata": {},
   "source": [
    "## Inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc3d6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"checkpoints/best_model.keras\"\n",
    "VOCAB_FILE = \"vocab.txt\"\n",
    "MAX_LEN = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43f26d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectorizer_from_vocab(vocab_file, max_len, max_tokens=None):\n",
    "    vocab = [line.rstrip(\"\\n\") for line in open(vocab_file, \"r\", encoding=\"utf-8\")]\n",
    "    # create new TextVectorization with same params as training\n",
    "    vectorizer = layers.TextVectorization(\n",
    "        max_tokens=max_tokens or len(vocab),\n",
    "        output_sequence_length=max_len,\n",
    "        standardize=\"lower_and_strip_punctuation\",\n",
    "        output_mode=\"int\"\n",
    "    )\n",
    "    # set vocabulary so it doesn't require adapt()\n",
    "    vectorizer.set_vocabulary(vocab)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2c6734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image_from_path(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img  # float32 H,W,3 scaled to [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5768c4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754759303.392367   53933 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3584 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-08-09 22:38:36.005729: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 9437184 exceeds 10% of free system memory.\n",
      "2025-08-09 22:38:36.799607: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 20480000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(MODEL_PATH, compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9babec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = load_vectorizer_from_vocab(VOCAB_FILE, max_len=MAX_LEN, max_tokens=None)\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "id_to_token = {i: t for i, t in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74aa841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_candidates = [\"<start>\", \"start\"]\n",
    "end_candidates = [\"<end>\", \"end\"]\n",
    "start_id = None\n",
    "end_id = None\n",
    "for w, idx in zip(vocab, range(len(vocab))):\n",
    "    lw = w.lower()\n",
    "    if start_id is None and lw in start_candidates:\n",
    "        start_id = idx\n",
    "    if end_id is None and lw in end_candidates:\n",
    "        end_id = idx\n",
    "# Fallbacks if your tokens are different\n",
    "if start_id is None:\n",
    "    # If you did not use explicit tokens, you can treat generation as starting from an \"empty\" prompt.\n",
    "    # Here we fall back to using token id for first non-zero token (less ideal).\n",
    "    start_id = 1\n",
    "if end_id is None:\n",
    "    end_id = None  # we'll rely on max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcb2f7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated caption: a street sign with a traffic sign on the street\n"
     ]
    }
   ],
   "source": [
    "def generate_caption_greedy(image_path, model, vectorizer, max_len=MAX_LEN):\n",
    "    img = preprocess_image_from_path(image_path)\n",
    "    img = tf.expand_dims(img, 0)  # batch dim\n",
    "\n",
    "    # initialize with start token\n",
    "    decoded = [start_id]\n",
    "    for i in range(max_len - 1):\n",
    "        # prepare decoder input (batch, seq_len)\n",
    "        dec_input = tf.expand_dims(tf.constant(decoded, dtype=tf.int32), 0)  # shape (1, cur_len)\n",
    "        # model expects (image_batch, caption_input). Provide current decoder tokens.\n",
    "        preds = model([img, dec_input], training=False)  # (1, seq_len, vocab)\n",
    "        # take logits at last timestep\n",
    "        logits = preds[0, -1, :]  # (vocab,)\n",
    "        next_id = int(tf.argmax(logits).numpy())\n",
    "        decoded.append(next_id)\n",
    "        if (end_id is not None) and (next_id == end_id):\n",
    "            break\n",
    "\n",
    "    # Convert token ids to tokens, strip start & end\n",
    "    # Remove the first token (start) and any end token and padding (id 0)\n",
    "    token_ids = decoded[1:]  # skip start\n",
    "    words = []\n",
    "    for tid in token_ids:\n",
    "        if tid == 0:\n",
    "            continue\n",
    "        tok = id_to_token.get(tid, \"\")\n",
    "        if end_id is not None and tid == end_id:\n",
    "            break\n",
    "        words.append(tok)\n",
    "    caption = \" \".join(w for w in words if w not in (\"<start>\", \"<end>\"))\n",
    "    # basic cleanup: collapse multiple spaces\n",
    "    caption = \" \".join(caption.split())\n",
    "    return caption\n",
    "\n",
    "# ---- Example usage ----\n",
    "image_path = \"COCO_train2014_000000000094.jpg\"\n",
    "caption = generate_caption_greedy(image_path, model, vectorizer)\n",
    "print(\"Generated caption:\", caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0a19f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
